# -*- coding: utf-8 -*-
"""NS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IT3LWI9GQ3FUU0nZf9CR0z3lzA2NYS0g
"""



pip install lxml_html_clean

pip install newspaper3k



pip install lxml_html_clean

import requests
from newspaper import Article  # Note: It's 'newspaper', not 'newspaper3k'
from bs4 import BeautifulSoup
import time

import requests
from newspaper import Article
from bs4 import BeautifulSoup
import time

pip install nltk

import nltk
nltk.download('punkt_tab')

import pandas as pd
import numpy as np
from textblob import TextBlob

# Function to fetch and parse news article metadata
def fetch_article_metadata(url):
    article = Article(url)
    article.download()
    article.parse()
    article.nlp()  # This allows for extracting the summary
    metadata = {
        'title': article.title,
        'summary': article.summary,
        'authors': article.authors,
        'published_date': article.publish_date,
        'keywords': article.keywords
        }
    return metadata


# Function to analyze sentiment
def analyze_sentiment(text):
    analysis = TextBlob(text)
    return analysis.sentiment.polarity

# Function to search for articles using a news API or scraping
def get_news_articles(company_name, num_articles=10):
    search_url = f"https://www.bbc.co.uk/search?q={company_name}&hl=en-US&gl=US&ceid=US%3Aen"
    response = requests.get(search_url)

    soup = BeautifulSoup(response.text, 'html.parser')

    articles = []

    # Finding article links in the Google News results page
    for link in soup.find_all('a', {'class': 'ssrcss-1yagzb7-PromoLink exn3ah95'}):
        article_url = "h"+link.get('href')[1:]  # Form the full URL


        try:
            article_metadata = fetch_article_metadata(article_url)
            articles.append(article_metadata)
            if len(articles) >= num_articles:
                break
        except Exception as e:
            print(f"Error processing article: {e}")
        time.sleep(2)  # To avoid hitting the server too frequently

    return articles

def main():
# Example: Get 10 news articles about a company (e.g., "Tesla")
  company_name = "Tesla"
  articles = get_news_articles(company_name, 10)
  news_list = []
  #print(articles)
# Display the extracted metadata
  for i, article in enumerate(articles, 1):
    print(f"Article {i}:")
    print(f"Title: {article['title']}")
    title=article['title']
    print(f"Summary: {article['summary']}")
    summary=article['summary']
    #print(f"Published Date: {article['published_date']}")
    #print(f"Authors: {', '.join(article['authors']) if article['authors'] else 'N/A'}")
    print(f"Keywords: {article['keywords']}")
    keywords_dt=article['keywords']
    sentiment = analyze_sentiment(summary)
    print("="*80)
    news_list.append((title,summary,keywords_dt,'Positive' if sentiment > 0 else 'Negative' if sentiment < 0 else 'Neutral'))

  df = pd.DataFrame(news_list, columns = ['Title', 'Summary', 'Keywords','Sentiment'])
#save the data in a csv file
  df.to_csv("newsdata.csv", index=False)


if __name__ == "__main__":
    main()

    #once you have scrapped all data, create a dataframe and assign the news_list variable to it.



A=pd.read_csv("/content/newsdata.csv")

A['Summary']

"""Sentiment Analysis Code"""

pip install textblob

from textblob import TextBlob

# Function to analyze sentiment
def analyze_sentiment(text):
    analysis = TextBlob(text)
    return analysis.sentiment.polarity

# Perform sentiment analysis on extracted articles
for item in A['Summary']:
      sentiment = analyze_sentiment(item)
      print(f"   Sentiment: {'Positive' if sentiment > 0 else 'Negative' if sentiment < 0 else 'Neutral'} ({sentiment})\n")

"""Text to Speech Code"""

pip install gtts

pip install googletrans==4.0.0-rc1 gtts

from gtts import gTTS
from googletrans import Translator

# Initialize translator
translator = Translator()

# English text
text_en = "Hello! How are you? Welcome to the Hindi Text-to-Speech demo."

# Translate English to Hindi
translated_text = translator.translate(text_en, src='en', dest='hi').text

# Convert to speech
tts = gTTS(text=translated_text, lang='hi')
tts.save("hindi_translation.mp3")

# Play audio
import os
os.system("start hindi_translation.mp3")

pip install pandas matplotlib nltk textblob

import nltk
nltk.download('vader_lexicon')

import pandas as pd
import nltk
import json
from nltk.sentiment import SentimentIntensityAnalyzer
from sklearn.feature_extraction.text import TfidfVectorizer

# Download necessary NLTK data
nltk.download("vader_lexicon")
nltk.download("punkt")

# Initialize Sentiment Analyzer
sia = SentimentIntensityAnalyzer()

A = "/content/newsdata.csv"
df = pd.read_csv(A)

# Load the CSV file (update the path as needed)
A = "/content/newsdata.csv"
try:
    df = pd.read_csv(A)
except FileNotFoundError:
    raise FileNotFoundError(f"Error: The file '{A}' was not found. Please check the path.")

# Ensure required columns exist
if "Title" not in df.columns or "Summary" not in df.columns:
    raise ValueError("CSV file must contain 'Title' and 'Summary' columns with news text and summaries.")

# Handle missing values
df = df.fillna("")

# Limit to 10 articles
df = df.head(10)

# Function to extract keywords from summary using TF-IDF
def extract_keywords(text, num_keywords=3):
    vectorizer = TfidfVectorizer(stop_words="english", max_features=50)
    tfidf_matrix = vectorizer.fit_transform([text])
    feature_array = vectorizer.get_feature_names_out()
    tfidf_scores = tfidf_matrix.toarray()[0]
    top_keywords = [feature_array[i] for i in tfidf_scores.argsort()[-num_keywords:][::-1]]
    return ", ".join(top_keywords)

# Perform sentiment analysis & extract key topics for each article
sentiment_results = []
for index, row in df.iterrows():
    Summary_text = str(row["Summary"])  # Using summary for impact extraction
    sentiment_score = sia.polarity_scores(Summary_text)
    keywords = extract_keywords(Summary_text)  # Extract key topics

    sentiment_results.append({
        "Title": f"Title {index + 1}",
        "Keywords": keywords,
        "Summary": Summary_text,  # Store summary for impact comparison
        "Positive": sentiment_score["pos"],
        "Neutral": sentiment_score["neu"],
        "Negative": sentiment_score["neg"],
        "Compound": sentiment_score["compound"]
    })

# Convert results to DataFrame
sentiment_df = pd.DataFrame(sentiment_results)

# Count sentiment distribution
sentiment_distribution = {
    "Positive": sum(1 for s in sentiment_df["Compound"] if s > 0),
    "Negative": sum(1 for s in sentiment_df["Compound"] if s < 0),
    "Neutral": sum(1 for s in sentiment_df["Compound"] if s == 0),
}

# Generate coverage differences dynamically using summaries
coverage_differences = []
for i in range(len(sentiment_df) - 1):
    article_1 = sentiment_df.iloc[i]
    article_2 = sentiment_df.iloc[i + 1]

    comparison_text = f"Article {i+1} highlights {article_1['Keywords']}, while Article {i+2} discusses {article_2['Keywords']}."
    impact_text = f"{article_1['Summary']} {article_2['Summary']}"  # Directly using summaries for impact

    coverage_differences.append({
        "Comparison": comparison_text,
        "Impact": impact_text
    })

# Final JSON output
sentiment_summary = {
    "Comparative Sentiment Score": {
        "Sentiment Distribution": sentiment_distribution,
        "Coverage Differences": coverage_differences
    }
}

# Save results to JSON file
with open("sentiment_comparison.json", "w") as json_file:
    json.dump(sentiment_summary, json_file, indent=4)
    print("Sentiment analysis results saved to sentiment_comparison.json")

# Print JSON result
print(json.dumps(sentiment_summary, indent=4))

pip install streamlit

